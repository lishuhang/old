# 清华自然语言处理科学家孙茂松：让算法懂得人类“常识”

![img](https://lishuhang.me/img/2019/11/sunmaosong.jpg)

*书航 11 月 4 日发于北京*

语音助手回答人们的指令、AI客服自动接听人类来电、部分替代人工的机器翻译……这些人工智能应用都属于自然语言处理（NLP）的范畴。目前，基于深度学习的NLP应用已经可以取得很好的效果，特别是在机器翻译方面，最终给人类呈现的答案越来越理想。

不过，目前的NLP应用都是基于深度学习的“黑箱”，通俗的说，就是我们不知道机器是怎么给我们这样的结果的，机器自己也不知道。它如鹦鹉学舌一般，并没有真正“懂得”经它处理的这句话是什么含义。

“黑箱”问题是AI研究的一个长期的焦点，从著名的“中文房间实验”提出开始就一直有各式各样的讨论和思考。航通社之前整理过[微软亚洲研究院院长洪小文在清华大学的演讲](http://mp.weixin.qq.com/s?__biz=MjM5Mjg1ODIxMQ==&mid=2650660600&idx=1&sn=54a005755541aff71138cb2ee0f18b7b&chksm=be9697f489e11ee293fa35f639fcf370a8bfca08686944000c7b0db4ea23417ba572dee03b7d&scene=21#wechat_redirect)，也提到了这方面的内容。

2019年10月31日，北京创建全球人工智能学术和创新最优生态的标志性学术活动“北京智源大会”在国家会议中心举办。清华大学计算机系教授，智源首席科学家孙茂松接受了航通社 / cnBeta 等媒体采访，提出了他眼中“打开黑箱”的独特方案。

## 解决AI可解释性问题的迄今最复杂尝试

孙茂松是自然语言处理领域国际领军人物，在自然语言处理的理论、方法及应用方面取得突出成果。数月前，孙教授被北京智源人工智能研究院聘请为“自然语言处理”重大研究方向首席科学家。

智源研究院是北京大力支持建设的新型研发机构，希望其通过机制体制创新，建设成为汇聚全球顶尖科学家研究力量，产出具有重要国际影响原创成果的重大战略平台。

当时还宣布了“北京智源-京东跨媒体对话智能联合实验室”揭牌。依托京东在电商零售领域的海量数据积累和超大算力，联合实验室将重点打造跨模态对话与人机交互领域的超大规模、开放领域、真实复杂场景数据集，打造智能零售场景下的前瞻性示范性应用。

![img](https://lishuhang.me/img/2019/11/baai-nlp-lab.jpg)

孙茂松希望解决机器翻译等“黑箱”NLP应用面对的最大挑战：看起来效果不错，其实算法本身对语义并没有任何自己的理解，这样系统在处理复杂语义的时候非常脆弱。“前门快到了，请从后门下车”等例子到现在也没有被攻克。

此前在上海的世界人工智能大会（WAIC）期间，[航通社 / cnBeta 也向IBM专家请教过同一问题](https://mp.weixin.qq.com/s?__biz=MjM5Mjg1ODIxMQ==&mid=2650660798&idx=1&sn=2e95acba1b3db3e63a61848dfc2fc4a9&scene=21#wechat_redirect)。IBM 倾向于用“黑箱”解释“黑箱”，通过同样是基于神经网络的办法，来解释 AI 模型的决策。2019 年 8 月，IBM 发布开源算法集合 AI Explainability 360，以增强算法的可解释性。

但孙茂松认为，黑箱是不得已而为之。黑箱在近几轮AI热潮兴起之初都起到了比较积极的作用，但发展到现在的问题也比较明显。他认为，重要的是让机器说出有常识、有逻辑的话，不能仅仅合乎语法，但在现实中却不成立，或者显得荒诞。

孙茂松团队将自己在智源实验室进行的NLP研究课题定为**“大数据+富知识双轮驱动的自然语言处理”**，需要建立一个可以被电脑认知的，可操作的人类知识库。显然与用“黑箱”解释“黑箱”相比，这是一条更复杂的道路。

## 建立“北京特色”的世界知识库和“常识库”

目前，有Wikidata、WordNet等一些行业先驱制作的知识库系统，企业也有自己做一些知识图谱，但要么不开放，要么不够被全人类认可，要么太过粗浅，“大而不强”。

![img](https://lishuhang.me/img/2019/11/wikidata.png)

孙茂松认为，NLP符合人类逻辑的关键是**让机器拥有所有人都懂得，且存在全球一致共识的“常识”**，为此建立的知识库更恰当的应该被称作是“常识库”。

WordNet、Wikidata等知识图谱是纯人工编辑，需要花费数十年的精力。孙茂松则希望他们可以在整合前人开放研究成果的基础上，**依赖现有的深度学习算法，通过拆解语料的句子结构**，将原本浩如烟海的语料库转化为各个元素之间的关系链，减少人类手工编辑的负担，并让项目可持续开展。

他希望自己的团队可以做出一个“体现北京特色的库，做的比较深入”，如果不能全做出来，至少要做出其中一部分。

航通社 / cnBeta 对这个“常识库”当中**如何定义和取舍“常识”**比较感兴趣。毕竟，有些人可能认为登月是阴谋论，可能还有其他关于地缘政治或者其他方面的分歧，在维基百科上也有出现基于意识形态冲突的激烈“编辑战”。

孙茂松希望做一个颗粒度并没有那么细的“常识”系统，也就是**只针对人类知识中比较稳定的核心部分**，超出这个常识范围的，就相当于观点，是允许有不同的。

> “你去餐馆，不管全世界哪个餐馆，你要点菜，上菜，吃完之后付账，不付账就跑人家肯定不干，这就属于常识。”

另一方面，观点是灵活的，难以穷尽的。观点的补足可以通过掌握“常识”基础上的大数据挖掘来做到。孙茂松团队会对构成“常识库”的原始语料和输出效果把关，里面不能包含事实错误。

目前，团队内的李涓子教授已经基于维基百科抽取了一些中英双语的世界知识库，这个库和其它一些清华NLP项目，已经开源[放在GitHub上](https://github.com/thunlp)了。清华NLP组开源得到的star，跟斯坦福Manning（世界上做NLP最好的组之一）的开源star差不多。

## 长远有助于减小训练算法的数据量

孙茂松认为，即使业界有使用小数据、乃至数据不上云，在本机运算以保护隐私的呼声，目前以此作为方向还很难，不容易出成果。因为小数据缺乏类似深度学习领域CNN、LITM、GPT2等这样通行的、公认的解决方案。

小数据目前只能用于有限领域，如 iOS 对用户自己的照片库做归纳，或者医院采集病人病历，这当然不可能大规模采集，必须基于小数据集研究。

但相应的，这样的小数据只能是**个案分析（case by case）专门制作解法**，不可复用；也就是只能基于专门领域的知识，和已经高度格式化的数据，才能产出对人有意义的结果。

基于小数据的NLP如果需要研发更大范围的通用算法，就必然涉及到对“常识”的预处理。从这个角度看，孙茂松团队试图制作的知识图谱，长远也会为摆脱运算对数据量的依赖起到帮助。